{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h49BdxPUCJ4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3586\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "f= open(r'stopwords_en.txt')\n",
    "fh = f.read()\n",
    "stop_words = []\n",
    "for word in fh:\n",
    "  stop_words.append(word)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "t41VJtJaHdf-"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as date_parser\n",
    "\n",
    "\n",
    "class CustomExtractor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stop_words)\n",
    "\n",
    "    def calculate_gravity_score(self, tag):\n",
    "        # Your custom logic to calculate the gravity score\n",
    "        # This example uses the length of text content as a score\n",
    "        text_content = tag.get_text(strip=True)\n",
    "        return len(text_content)\n",
    "\n",
    "    def walk_siblings(self, node):\n",
    "        # Iterate over siblings\n",
    "        siblings = []\n",
    "        for sibling in node.find_all_next():\n",
    "            if sibling.name and sibling.name != 'text':\n",
    "                siblings.append(sibling)\n",
    "        return siblings\n",
    "\n",
    "    def is_highlink_density(self, node):\n",
    "\n",
    "        # For simplicity, this example checks if the node contains more than 5 links\n",
    "        links = node.find_all('a')\n",
    "        return len(links) > 5\n",
    "\n",
    "    def is_boostable(self, node):\n",
    "        para = \"p\"\n",
    "        steps_away = 0\n",
    "        minimum_stopword_count = 5\n",
    "        max_stepsaway_from_node = 3\n",
    "\n",
    "        nodes = self.walk_siblings(node)\n",
    "        for current_node in nodes:\n",
    "            current_node_tag = current_node.name\n",
    "            if current_node_tag == para:\n",
    "                if steps_away >= max_stepsaway_from_node:\n",
    "                    return False\n",
    "                paragraph_text = current_node.get_text(strip=True)\n",
    "                word_stats = self.get_stopword_count(paragraph_text)\n",
    "                if word_stats > minimum_stopword_count:\n",
    "                    return True\n",
    "                steps_away += 1\n",
    "        return False\n",
    "\n",
    "    def get_stopword_count(self, text):\n",
    "        words = [word.lower() for word in text.split()]\n",
    "        sm = 0\n",
    "        for word in words:\n",
    "          if(word in self.stopwords):\n",
    "            sm+=1\n",
    "        return sm\n",
    "\n",
    "\n",
    "    def calculate_best_node(self, soup):\n",
    "        top_node = None\n",
    "        top_node_score = 0\n",
    "        nodes_to_check = self.nodes_to_check(soup)\n",
    "        # print(nodes_to_check)\n",
    "        starting_boost = 1.0\n",
    "        cnt = 0\n",
    "        i = 0\n",
    "        parent_nodes = []\n",
    "        nodes_with_text = []\n",
    "\n",
    "\n",
    "        for node in nodes_to_check:\n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "\n",
    "            high_link_density = self.is_highlink_density(node)\n",
    "            if word_stats >= 2 and not high_link_density:\n",
    "                nodes_with_text.append(node)\n",
    "\n",
    "        nodes_number = len(nodes_with_text)\n",
    "        negative_scoring = 0\n",
    "        bottom_negativescore_nodes = nodes_number * 0.25\n",
    "\n",
    "\n",
    "        for node in nodes_with_text:\n",
    "\n",
    "            boost_score = 0.0\n",
    "            if self.is_boostable(node):\n",
    "                if cnt >= 0:\n",
    "                    boost_score = 1.0 / starting_boost * 50\n",
    "                    starting_boost += 1\n",
    "\n",
    "            if nodes_number > 15:\n",
    "                if (nodes_number - i) <= bottom_negativescore_nodes:\n",
    "                    booster = bottom_negativescore_nodes - (nodes_number - i)\n",
    "                    boost_score = -pow(booster, 2)\n",
    "                    negscore = abs(boost_score) + negative_scoring\n",
    "                    if negscore > 40:\n",
    "                        boost_score = 5.0\n",
    "\n",
    "\n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "            upscore = word_stats + boost_score\n",
    "\n",
    "            parent_node = node.parent\n",
    "            self.update_score(parent_node, upscore)\n",
    "            self.update_node_count(parent_node, 1)\n",
    "\n",
    "            if parent_node not in parent_nodes:\n",
    "                parent_nodes.append(parent_node)\n",
    "\n",
    "            parent_parent_node = parent_node.parent\n",
    "            if parent_parent_node is not None:\n",
    "                self.update_node_count(parent_parent_node, 1)\n",
    "                self.update_score(parent_parent_node, upscore / 2)\n",
    "                if parent_parent_node not in parent_nodes:\n",
    "                    parent_nodes.append(parent_parent_node)\n",
    "            cnt += 1\n",
    "            i += 1\n",
    "\n",
    "        for e in parent_nodes:\n",
    "            score = self.get_score(e)\n",
    "\n",
    "            if score > top_node_score:\n",
    "                top_node = e\n",
    "                top_node_score = score\n",
    "\n",
    "            if top_node is None:\n",
    "                top_node = e\n",
    "\n",
    "        return top_node\n",
    "\n",
    "    def nodes_to_check(self, soup):\n",
    "        nodes_to_check = []\n",
    "        for tag in ['p', 'pre', 'td','div']:\n",
    "            items = soup.find_all(tag)\n",
    "            nodes_to_check += items\n",
    "        return nodes_to_check\n",
    "\n",
    "    def update_score(self, node, score):\n",
    "        if 'score' not in node:\n",
    "            node['score'] = len(node.get_text(strip=True))\n",
    "        node['score'] += score\n",
    "\n",
    "    def update_node_count(self, node, count):\n",
    "        if 'count' not in node:\n",
    "            node['count'] = 0\n",
    "        node['count'] += count\n",
    "\n",
    "    def get_score(self, node):\n",
    "        return node.get('score', 0)\n",
    " \n",
    "    def get_authors(self, doc):\n",
    "        def contains_digits(d):\n",
    "            for char in d:\n",
    "                if char.isdigit():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        def unique_list(lst):\n",
    "            count = {}\n",
    "            list = []\n",
    "            for item in lst:\n",
    "                if item.lower() in count:\n",
    "                    continue\n",
    "                count[item.lower()] = 1\n",
    "                list.append(item.title())\n",
    "            return list\n",
    "    \n",
    "        def parse(str):\n",
    "            \n",
    "            str = ''.join(char for char in str if char != '<' and char != '>')\n",
    "\n",
    "            str = re.sub(r'\\b(by:|from:|news|media|bureau)\\b', '', str, flags=re.IGNORECASE).strip()\n",
    "\n",
    "            names = [s.strip() for s in re.split(r\"[^\\w\\'\\-\\.]\", str) if s]\n",
    "\n",
    "    \n",
    "            authors = []\n",
    "            current = []\n",
    "            delimiters = ['and', ',', '']\n",
    "    \n",
    "            for name in names:\n",
    "                if name in delimiters:\n",
    "                    if len(current) > 0:\n",
    "                        authors.append(' '.join(current))\n",
    "                        current = []\n",
    "                elif not contains_digits(name):\n",
    "                    current.append(name)\n",
    "    \n",
    "            valid_name = (len(current) >= 2)\n",
    "            if valid_name:\n",
    "                authors.append(' '.join(current))\n",
    "    \n",
    "            return authors\n",
    "        patterns=[re.compile(r'(?:author.*?name|name.*?author)', re.IGNORECASE)]\n",
    "        attributes = ['name', 'rel', 'itemprop', 'class', 'id']\n",
    "        variables = ['author', 'byline', 'dc.creator', 'byl','author-name','author-content','aaticleauthor_name','group-info']\n",
    "        # pattern = re.compile(r'(?:written by|edited by|published by)[\\s:-]+([A-Za-z]+(?:\\s+[A-Za-z]+)*)', re.IGNORECASE)\n",
    "        pattern= re.compile(r'(?:written by|edited by|published by)[\\s:-]+([A-Za-z]+(?:\\s+[A-Za-z]+)?)', re.IGNORECASE)\n",
    "\n",
    "           \n",
    "        matches = []\n",
    "        authors = []\n",
    "    \n",
    "        for attr in attributes:\n",
    "            for val in variables+patterns:\n",
    "                found = doc.find_all(attrs={attr: val})\n",
    "                matches.extend(found)\n",
    "        \n",
    "\n",
    "        for match in matches:\n",
    "            content = ''\n",
    "            if match.tag == 'meta':\n",
    "                content_value = match.get('content')\n",
    "                if len(content_value) > 0:\n",
    "                    content = content_value[0]\n",
    "            else:\n",
    "                content = match.get_text() or ''\n",
    "            if len(content) > 0:\n",
    "                authors.extend(parse(content))\n",
    "        \n",
    "        match = pattern.search(doc.get_text())\n",
    "\n",
    "        if match:\n",
    "            authors.extend(parse(match.group(1)))\n",
    "    \n",
    "        return unique_list(authors)\n",
    "    \n",
    "    def publishing_date(self,doc):\n",
    "        def parse_date(str):\n",
    "            if str:\n",
    "                try:\n",
    "                    return date_parser(str)\n",
    "                except (ValueError, OverflowError, AttributeError, TypeError):\n",
    "                    return None\n",
    "    \n",
    "        \n",
    "        STRICT_DATE_REGEX = re.compile(r'\\/(\\d{4})\\/(\\d{2})\\/(\\d{2})\\/')\n",
    "        date_pattern=re.compile(r'([a-zA-Z]+?\\s?\\d{1,2}\\s?\\,?\\s?\\d{4}\\s?,?\\s?\\d{1,2}:\\d{2}(?:\\s?[APMapm]{2}\\s?| IST\\s?))|(\\d{2}\\s*[a-zA-Z]+\\s*\\d{4})\\s*,?\\s*(\\d{1,2}:\\d{2}(?:\\s?[APMapm]{2}\\s?| IST\\s?))', re.IGNORECASE)\n",
    "        # date_pattern=re.compile(r'([a-zA-Z]+?\\s?\\d{1,2}\\s?\\,?\\s?\\d{4}\\s?,?\\s?\\d{2}:\\d{2}\\s(?:AM|PM|IST))|(\\d{2}\\s*[a-zA-Z]+\\s*\\d{4})\\s*,?\\s*(\\d{2}:\\d{2}(?:AM|PM|IST))', re.IGNORECASE)\n",
    "\n",
    "        # date_pattern=re.compile(r'([a-zA-Z]+?\\s?\\d{1,2}\\s?\\,?\\s?\\d{4}\\s?,?\\s\\d{2}:\\d{2}\\s(?:AM|PM))|(\\d{2}\\s*[a-zA-Z]+\\s*\\d{4})\\s*,?\\s*(\\d{2}:\\d{2}[APMapm]{2})', re.IGNORECASE)\n",
    "        # date_pattern = re.compile(r'([a-zA-Z]+?\\s?\\d{1,2}\\s?\\,?\\s?\\d{4}\\s?,?\\s\\d{2}:\\d{2}\\s(?:AM|PM))', re.IGNORECASE)\n",
    "        # /date_pattern =re.compile(r'(\\d{2}\\s*[a-zA-Z]+\\s*\\d{4})\\s*,?\\s*(\\d{2}:\\d{2}[APMapm]{2})', re.IGNORECASE)\n",
    "\n",
    "        # Extract text content from the entire HTML document\n",
    "        html_text = doc.get_text()\n",
    "\n",
    "        date_match = STRICT_DATE_REGEX.search(html_text)\n",
    "        if date_match:\n",
    "            date_str = date_match.group(0)\n",
    "            return parse_date(date_str)\n",
    "\n",
    "        # If strict date format is not found, try the alternative format\n",
    "        alt_date_match = date_pattern.search(html_text)\n",
    "        if alt_date_match:\n",
    "            date_str = alt_date_match.group(1)\n",
    "            return parse_date(date_str)\n",
    "\n",
    "        \n",
    "        patterns = re.compile(r'(?:article.*publish|publish.*article|\\bdate\\b|\\btime\\b)')\n",
    "        date_tags = [\n",
    "            {'attribute': ('property', 'rnews:datePublished'), 'content': 'content'},\n",
    "            {'attribute': ('property', 'article:published_time'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'OriginalPublicationDate'), 'content': 'content'},\n",
    "            {'attribute': ('itemprop', 'datePublished'), 'content': 'datetime'},\n",
    "            {'attribute': ('itemprop', 'dateModified'), 'content': 'datetime'},\n",
    "            {'attribute': ('property', 'og:published_time'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'article_date_original'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'publication_date'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'sailthru.date'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'PublishDate'), 'content': 'content'},\n",
    "            {'attribute': ('pubdate', 'pubdate'), 'content': 'datetime'},\n",
    "            {'attribute': ('name', 'publish_date'), 'content': 'content'},\n",
    "        ]\n",
    "    \n",
    "        \n",
    "        for tags in date_tags:\n",
    "            meta_tags = doc.find_all(attrs={tags['attribute'][0]: tags['attribute'][1]})\n",
    "            if meta_tags:\n",
    "                str = meta_tags[0].get(tags['content'])\n",
    "                datetime = parse_date(str)\n",
    "                if datetime:\n",
    "                    return datetime\n",
    "        additional_date_tag = doc.find('div', class_=lambda c: c and patterns.search(c))\n",
    "        if additional_date_tag:\n",
    "            str = additional_date_tag.get_text(strip=True)\n",
    "            match = date_pattern.search(str)\n",
    "            if match:\n",
    "                date_str, time_str = match.groups()\n",
    "\n",
    "                # Combine date and time, then parse using dateutil.parser\n",
    "                datetime_str = f\"{date_str} {time_str}\"\n",
    "                datetime_obj = date_parser(datetime_str) \n",
    "            return datetime_obj\n",
    "    \n",
    "        # If none of the strategies work, return None\n",
    "        return None\n",
    "    def split_title(self,title, splitter, hint=None):\n",
    "        \"\"\"Split the title to best part possible\"\"\"\n",
    "        large_text_length = 0\n",
    "        large_text_index = 0\n",
    "        title_pieces = title.split(splitter)\n",
    "        if hint and hint!='':\n",
    "            filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "            hint = filter_regex.sub('', hint).lower()\n",
    "    \n",
    "        # find the largest title piece\n",
    "        for i, title_piece in enumerate(title_pieces):\n",
    "            current = title_piece.strip()\n",
    "            #Immediately break if any part matches\n",
    "            if hint and hint in filter_regex.sub('', current).lower():\n",
    "                large_text_index = i\n",
    "                break\n",
    "            if len(current) > large_text_length:\n",
    "                large_text_length = len(current)\n",
    "                large_text_index = i\n",
    "    \n",
    "    #     Even if no part matches with hint(h1) if prints simply the longest part as the parts\n",
    "    #     are usually of independent meaning\n",
    "        title = title_pieces[large_text_index]\n",
    "        return title    \n",
    "    def get_title(self,soup):\n",
    "        \"\"\"Explicit rules:\n",
    "        1. title == h1, no need to split\n",
    "        2. h1 similar to og:title, use h1\n",
    "        3. title contains h1, title contains og:title, len(h1) > len(og:title), use h1\n",
    "        4. title starts with og:title, use og:title\n",
    "        5. use title, after splitting\n",
    "        \"\"\"\n",
    "        title = ''\n",
    "        title_element = soup.title\n",
    "    \n",
    "        # no title found\n",
    "        if title_element is None or len(title_element) == 0:\n",
    "            print(\"Error\")\n",
    "            return title\n",
    "    \n",
    "        # title elem found\n",
    "        title_text = title_element.text\n",
    "        used_delimeter = False\n",
    "    \n",
    "    #     title from h1\n",
    "            # - extract the longest text from all h1 elements\n",
    "        # - too short texts (fewer than 2 words) are discarded\n",
    "        # - clean double spaces\n",
    "    #     h1_element = soup.find_all('h1')[0]\n",
    "    #     title_text_h1 = h1_element.text\n",
    "        title_text_h1=''\n",
    "        title_element_h1_list = soup.find_all('h1')\n",
    "        title_text_h1_list = [tag.get_text(strip=True) for tag in title_element_h1_list]\n",
    "        if title_text_h1_list:\n",
    "            title_text_h1_list.sort(key=len, reverse=True)\n",
    "            #longest title\n",
    "            title_text_h1 = title_text_h1_list[0]\n",
    "            # clean double spaces\n",
    "            title_text_h1 = ' '.join([x for x in title_text_h1.split() if x])\n",
    "        #title from meta tag(not user-visible)\n",
    "        meta_tag_content = soup.find({'meta': {'property': 'og:title'}})\n",
    "        if not meta_tag_content:\n",
    "            meta_tag_content = soup.find({'meta': {'name': 'og:title'}})\n",
    "        title_text_meta = meta_tag_content.get('content', '')  # Empty string if no meta tag found\n",
    "        # Further filtering of unwanted characters\n",
    "        # Alphanumeric characters, punctuation and alphanumeric\n",
    "        filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "        filter_title_text = filter_regex.sub('', title_text).lower()\n",
    "        filter_title_text_h1 = filter_regex.sub('', title_text_h1).lower()\n",
    "        filter_title_text_meta = filter_regex.sub('', title_text_meta).lower()\n",
    "        \n",
    "        # Case1: If both matches don't do anything\n",
    "        if title_text_h1 == title_text:\n",
    "            used_delimeter = True\n",
    "        # Case2: h1 and meta tag matches(either of h1 or meta)\n",
    "        elif filter_title_text_h1 and filter_title_text_h1 == filter_title_text_meta:\n",
    "            title_text = title_text_h1\n",
    "            used_delimeter = True\n",
    "        # Case3: If both h1 and meta are a substring of title_text(use h1)\n",
    "        elif filter_title_text_h1 and filter_title_text_h1 in filter_title_text and filter_title_text_meta in filter_title_text  and len(title_text_h1) > len(title_text_meta):\n",
    "            title_text = title_text_h1\n",
    "            used_delimeter = True\n",
    "        # Case4: If title_text startswith meta text(replace with meta)\n",
    "        elif filter_title_text_meta and filter_title_text_meta != filter_title_text and filter_title_text.startswith(filter_title_text_meta):\n",
    "            title_text = title_text_meta\n",
    "            used_delimeter = True\n",
    "        \n",
    "        # If none of the above condition is matched, means a delimiter must be present between them\n",
    "        # Now individually parts separated by delimiter has to be checked and now we check with h1 tag only(no meta tag)-Observation based\n",
    "        if not used_delimeter and '|' in title_text:\n",
    "            title_text = self.split_title(title_text, '|', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with -\n",
    "        if not used_delimeter and '-' in title_text:\n",
    "            title_text = self.split_title(title_text, '-', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with _\n",
    "        if not used_delimeter and '_' in title_text:\n",
    "            title_text = self.split_title(title_text, '_', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with /\n",
    "        if not used_delimeter and '/' in title_text:\n",
    "            title_text = self.split_title(title_text, '/', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with »\n",
    "        if not used_delimeter and ' » ' in title_text:\n",
    "            title_text = self.split_title(title_text, ' » ', title_text_h1)\n",
    "            used_delimeter = True\n",
    "        return title_text\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDt5gevywMRA",
    "outputId": "2ee5c5f8-a1e9-42ce-dcba-260327dbf757",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors  are\n",
      "No authors found.\n",
      "date of publication: 2024-01-15 19:38:00\n",
      "title  is/are\n",
      "Enter the new era of mobile technology, as Samsung is all set to introduce the revolutionary Galaxy AI\n",
      "No best node found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "url =\"https://www.gadgetsnow.com/gn-advertorial/enter-the-new-era-of-mobile-technology-as-samsung-is-all-set-to-introduce-the-revolutionary-galaxy-ai/articleshow/106867089.cms?upcache=2&_gl=1*1ncy8wg*_ga*MzI4MzUwNDA5LjE2OTI1NTY4MTQ.*_ga_FCN624MN68*MTcwNTkxOTU1Ny4xMS4xLjE3MDU5MTk4MTIuNjAuMC4w\"\n",
    "# proxies = {\n",
    "# \"http\": \"http://scraperapi:8355bf750256f87924cb321115d06996@proxy-server.scraperapi.com:8001\"\n",
    "# }\n",
    "# headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "# response = requests.get(url,headers=headers,proxies=proxies,verify=False)\n",
    "API_KEY = \"8355bf750256f87924cb321115d06996\"\n",
    "params = {'api_key': API_KEY, 'url': url}\n",
    "response = requests.get('http://api.scraperapi.com/', params=urlencode(params))\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# print(soup)\n",
    "# Create an instance of CustomExtractor\n",
    "extractor = CustomExtractor()\n",
    "\n",
    "\n",
    "authors_list=extractor.get_authors(soup);\n",
    "print(\"authors  are\")\n",
    "\n",
    "if authors_list:\n",
    "    \n",
    "    for author in authors_list:\n",
    "        print(author)\n",
    "       \n",
    "else:\n",
    "    print(\"No authors found.\")\n",
    "date=extractor.publishing_date(soup)\n",
    "\n",
    "print(\"date of publication:\",date)\n",
    "\n",
    "title_list=extractor.get_title(soup);\n",
    "print(\"title  is/are\")\n",
    "\n",
    "print(extractor.get_title(soup))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate best node based on custom gravity scores\n",
    "best_node = extractor.calculate_best_node(soup)\n",
    "\n",
    "# Now, you can access the best node and its gravity score\n",
    "if best_node:\n",
    "    print(f\" Gravity Score: {best_node.get('score', 0)}\")\n",
    "    print(best_node.get_text(strip=True))\n",
    "else:\n",
    "    print(\"No best node found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLHXth5VCnoBeFhz4tcqmD",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
