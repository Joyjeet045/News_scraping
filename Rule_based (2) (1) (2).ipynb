{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "h49BdxPUCJ4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3586\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import import_ipynb\n",
    "import parser\n",
    "from parser import HTMLParser\n",
    "\n",
    "f= open(r'stopwords_en.txt')\n",
    "\n",
    "fh = f.read()\n",
    "stop_words = []\n",
    "for word in fh:\n",
    "  stop_words.append(word)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "t41VJtJaHdf-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:364: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:364: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_51755/3346883351.py:364: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  elif filter_title_text_h1 and filter_title_text_h1 in filter_title_text and filter_title_text_meta is not '' and filter_title_text_meta in filter_title_text and len(title_text_h1) > len(title_text_meta):\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as date_parser\n",
    "\n",
    "\n",
    "class CustomExtractor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = stop_words\n",
    "\n",
    "    def calculate_gravity_score(self, tag):\n",
    "        # Your custom logic to calculate the gravity score\n",
    "        # This example uses the length of text content as a score\n",
    "        text_content = tag.get_text(strip=True)\n",
    "        return len(text_content)\n",
    "\n",
    "    def walk_siblings(self, node):\n",
    "        # Iterate over siblings\n",
    "        siblings = []\n",
    "        for sibling in node.find_all_next():\n",
    "            if sibling.name:\n",
    "                siblings.append(sibling)\n",
    "        return siblings\n",
    "\n",
    "    def is_highlink_density(self, node):\n",
    "\n",
    "        # For simplicity, this example checks if the node contains more than 5 links\n",
    "        links = node.find_all('a')\n",
    "        return len(links) > 25\n",
    "\n",
    "    def is_boostable(self, node):\n",
    "        para = \"p\"\n",
    "        steps_away = 0\n",
    "        minimum_stopword_count = 5\n",
    "        max_stepsaway_from_node = 3\n",
    "\n",
    "        nodes = self.walk_siblings(node)\n",
    "        for current_node in nodes:\n",
    "            current_node_tag = current_node.name\n",
    "            if current_node_tag == para:\n",
    "                if steps_away >= max_stepsaway_from_node:\n",
    "                    return False\n",
    "                paragraph_text = current_node.get_text(strip=True)\n",
    "                word_stats = self.get_stopword_count(paragraph_text)\n",
    "                if word_stats > minimum_stopword_count:\n",
    "                    return True\n",
    "                steps_away += 1\n",
    "        return False\n",
    "\n",
    "    def get_stopword_count(self, text):\n",
    "        words = [word.lower() for word in text.split()]\n",
    "        sm = 0\n",
    "        for word in words:\n",
    "          if(word in self.stopwords):\n",
    "            sm+=1\n",
    "        return sm\n",
    "\n",
    "\n",
    "    def calculate_best_node(self, soup):\n",
    "        top_node = None\n",
    "        top_node_score = 0\n",
    "        nodes_to_check = self.nodes_to_check(soup)\n",
    "        # print(nodes_to_check)\n",
    "        starting_boost = 1.0\n",
    "        cnt = 0\n",
    "        i = 0\n",
    "        parent_nodes = []\n",
    "        nodes_with_text = []\n",
    "\n",
    "\n",
    "        for node in nodes_to_check:\n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "#             print(node.get_text(strip=True))\n",
    "            high_link_density = self.is_highlink_density(node)\n",
    "            if word_stats >= 2 and not high_link_density:\n",
    "                nodes_with_text.append(node)\n",
    "\n",
    "        nodes_number = len(nodes_with_text)\n",
    "        negative_scoring = 0\n",
    "        bottom_negativescore_nodes = nodes_number * 0.25\n",
    "\n",
    "\n",
    "        for node in nodes_with_text:\n",
    "\n",
    "            boost_score = len(node.find_all('p'))\n",
    "            \n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "            temp = 0\n",
    "            try:\n",
    "                temp = len(node.content)\n",
    "            except:\n",
    "                pass\n",
    "            upscore = word_stats + boost_score - abs((temp - 3*boost_score))**0.25\n",
    "            \n",
    "#             if(node.has_attr('class')):\n",
    "#                 print(node['class'])\n",
    "#             print(node.name,\" \",word_stats,\" \",boost_score)\n",
    "            \n",
    "            parent_node = node.parent\n",
    "            self.update_score(node,upscore)\n",
    "            \n",
    "            x = node\n",
    "            if(node.name == 'p'):\n",
    "                for i in range(5):\n",
    "                    x = x.parent\n",
    "                    self.update_score(x,upscore*((0.8)**i))\n",
    "\n",
    "            if parent_node not in parent_nodes:\n",
    "                parent_nodes.append(parent_node)\n",
    "            \n",
    "            parent_parent_node = parent_node.parent\n",
    "            if parent_parent_node is not None:\n",
    "                self.update_node_count(parent_parent_node, 1)\n",
    "        \n",
    "                if parent_parent_node not in parent_nodes:\n",
    "                    parent_nodes.append(parent_parent_node)\n",
    "            cnt += 1\n",
    "            i += 1\n",
    "            \n",
    "\n",
    "        for e in parent_nodes:\n",
    "            score = self.get_score(e)\n",
    "\n",
    "            if score > top_node_score:\n",
    "#                 print(node.name,\" \",node['score'])\n",
    "                top_node = e\n",
    "                top_node_score = score\n",
    "\n",
    "            if top_node is None:\n",
    "                top_node = e\n",
    "\n",
    "        return top_node\n",
    "\n",
    "    def nodes_to_check(self, soup):\n",
    "        nodes_to_check = []\n",
    "        for tag in ['div','p', 'pre', 'td']:\n",
    "            items = soup.find_all(tag)\n",
    "            nodes_to_check += items\n",
    "#         print(nodes_to_check)\n",
    "        return nodes_to_check\n",
    "    \n",
    "\n",
    "    def update_score(self, node, score):\n",
    "        if 'score' not in node:\n",
    "            node['score'] = len(node.get_text(strip=True))**(0.5)\n",
    "        node['score'] += score\n",
    "\n",
    "    def update_node_count(self, node, count):\n",
    "        if 'count' not in node:\n",
    "            node['count'] = 0\n",
    "        \n",
    "        node['count'] += count\n",
    "\n",
    "    def get_score(self, node):\n",
    "        return node.get('score', 0)\n",
    " \n",
    "    def get_authors(self, doc):\n",
    "        def contains_digits(d):\n",
    "            for char in d:\n",
    "                if char.isdigit():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        def unique_list(lst):\n",
    "            count = {}\n",
    "            list = []\n",
    "            for item in lst:\n",
    "                if item.lower() in count:\n",
    "                    continue\n",
    "                count[item.lower()] = 1\n",
    "                list.append(item.title())\n",
    "            return list\n",
    "    \n",
    "        def parse(str):\n",
    "            \n",
    "            str = ''.join(char for char in str if char != '<' and char != '>')\n",
    "\n",
    "            str = str.replace('By:', '').replace('From:', '').strip()\n",
    "            \n",
    "            names = [s.strip() for s in re.split(r\"[^\\w\\'\\-\\.]\", str) if s]\n",
    "\n",
    "    \n",
    "            authors = []\n",
    "            current = []\n",
    "            delimiters = ['and', ',', '']\n",
    "    \n",
    "            for name in names:\n",
    "                if name in delimiters:\n",
    "                    if len(current) > 0:\n",
    "                        authors.append(' '.join(current))\n",
    "                        current = []\n",
    "                elif not contains_digits(name):\n",
    "                    current.append(name)\n",
    "    \n",
    "            valid_name = (len(current) >= 2)\n",
    "            if valid_name:\n",
    "                authors.append(' '.join(current))\n",
    "    \n",
    "            return authors\n",
    "        patterns=[re.compile(r'(?:author.*?name|name.*?author)', re.IGNORECASE)]\n",
    "        attributes = ['name', 'rel', 'itemprop', 'class', 'id']\n",
    "        variables = ['author', 'byline', 'dc.creator', 'byl','group-info']\n",
    "        matches = []\n",
    "        authors = []\n",
    "    \n",
    "        for attr in attributes:\n",
    "            for val in variables+patterns:\n",
    "                found = doc.find_all(attrs={attr: val})\n",
    "                matches.extend(found)\n",
    "        \n",
    "\n",
    "        for match in matches:\n",
    "            content = ''\n",
    "            if match.tag == 'meta':\n",
    "                content_value = match.get('content')\n",
    "                if len(content_value) > 0:\n",
    "                    content = content_value[0]\n",
    "            else:\n",
    "                content = match.get_text() or ''\n",
    "            if len(content) > 0:\n",
    "                authors.extend(parse(content))\n",
    "    \n",
    "        return unique_list(authors)\n",
    "    \n",
    "    def publishing_date(self, url, doc):\n",
    "        def parse_date(str):\n",
    "            if str:\n",
    "                try:\n",
    "                    return date_parser(str)\n",
    "                except (ValueError, OverflowError, AttributeError, TypeError):\n",
    "                    return None\n",
    "    \n",
    "        \n",
    "        STRICT_DATE_REGEX = re.compile(r'\\/(\\d{4})\\/(\\d{2})\\/(\\d{2})\\/')\n",
    "        date_pattern = re.compile(r'(\\d{2} [a-zA-Z]{3} \\d{4}) (\\d{2}:\\d{2}[APMapm]{2})')\n",
    "        date_match = STRICT_DATE_REGEX.search(url)\n",
    "        if date_match:\n",
    "            str = date_match.group(0)\n",
    "            datetime = parse_date(str)\n",
    "            if datetime:\n",
    "                return datetime\n",
    "    \n",
    "        \n",
    "        date_tags = [\n",
    "            {'attribute': ('property', 'rnews:datePublished'), 'content': 'content'},\n",
    "            {'attribute': ('property', 'article:published_time'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'OriginalPublicationDate'), 'content': 'content'},\n",
    "            {'attribute': ('itemprop', 'datePublished'), 'content': 'datetime'},\n",
    "            {'attribute': ('property', 'og:published_time'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'article_date_original'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'publication_date'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'sailthru.date'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'PublishDate'), 'content': 'content'},\n",
    "            {'attribute': ('pubdate', 'pubdate'), 'content': 'datetime'},\n",
    "            {'attribute': ('name', 'publish_date'), 'content': 'content'},\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        patterns = re.compile(r'(?:article.*publish|publish.*article|\\bdate\\b|\\btime\\b)')\n",
    "\n",
    "        for tags in date_tags:\n",
    "            meta_tags = doc.find_all(attrs={tags['attribute'][0]: tags['attribute'][1]})\n",
    "            if meta_tags:\n",
    "                str = meta_tags[0].get(tags['content'])\n",
    "                datetime = parse_date(str)\n",
    "                if datetime:\n",
    "                    return datetime\n",
    "        \n",
    "        additional_date_tag = doc.find('div', class_=lambda c: c and patterns.search(c))\n",
    "        if additional_date_tag:\n",
    "            str = additional_date_tag.get_text(strip=True)\n",
    "            match = date_pattern.search(str)\n",
    "            if match:\n",
    "                date_str, time_str = match.groups()\n",
    "\n",
    "                # Combine date and time, then parse using dateutil.parser\n",
    "                datetime_str = f\"{date_str} {time_str}\"\n",
    "                datetime_obj = date_parser(datetime_str) \n",
    "            return datetime_obj\n",
    "        \n",
    "\n",
    "    \n",
    "        # If none of the strategies work, return None\n",
    "        return None\n",
    "    def split_title(self,title, splitter, hint=None):\n",
    "        \"\"\"Split the title to best part possible\"\"\"\n",
    "        large_text_length = 0\n",
    "        large_text_index = 0\n",
    "        title_pieces = title.split(splitter)\n",
    "        if hint and hint!='':\n",
    "            filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "            hint = filter_regex.sub('', hint).lower()\n",
    "    \n",
    "        # find the largest title piece\n",
    "        for i, title_piece in enumerate(title_pieces):\n",
    "            current = title_piece.strip()\n",
    "            #Immediately break if any part matches\n",
    "            if hint and hint in filter_regex.sub('', current).lower():\n",
    "                large_text_index = i\n",
    "                break\n",
    "            if len(current) > large_text_length:\n",
    "                large_text_length = len(current)\n",
    "                large_text_index = i\n",
    "    \n",
    "    #     Even if no part matches with hint(h1) if prints simply the longest part as the parts\n",
    "    #     are usually of independent meaning\n",
    "        title = title_pieces[large_text_index]\n",
    "        return title    \n",
    "    def get_title(self, parser):\n",
    "        \"\"\"Explicit rules:\n",
    "        1. title == h1, no need to split\n",
    "        2. h1 similar to og:title, use h1\n",
    "        3. title contains h1, title contains og:title, len(h1) > len(og:title), use h1\n",
    "        4. title starts with og:title, use og:title\n",
    "        5. use title, after splitting\n",
    "        \"\"\"\n",
    "        title = ''\n",
    "        title_elements = parser.findalltags('title')\n",
    "\n",
    "        # no title found\n",
    "        if title_elements is None or len(title_elements) == 0:\n",
    "            print(\"Error\")\n",
    "            return title\n",
    "        \n",
    "        # title elem found\n",
    "        title_text = parser.textbytag(title_elements[0]['tag'])[0]\n",
    "        used_delimeter = False\n",
    "        \n",
    "       \n",
    "        # title from h1\n",
    "        title_text_h1 = ''\n",
    "        title_element_h1_list = parser.findalltags('h1')\n",
    "        title_text_h1_list = [parser.textbytag(tag_name=tag['tag'],attribute_name=tag['attributes'].get('class', None))[0] for tag in title_element_h1_list]\n",
    "\n",
    "#         print(title_text_h1_list)\n",
    "        if title_text_h1_list:\n",
    "            title_text_h1_list.sort(key=len, reverse=True)\n",
    "            title_text_h1 = ' '.join([x for x in title_text_h1_list[0].split() if x])\n",
    "#         print(title_text_h1)\n",
    "#         print(title_text)\n",
    "        \n",
    "        # title from meta tag (not user-visible)\n",
    "        meta_tag_content = parser.findallbyattribute(attribute_name='property', attribute_value='og:title')\n",
    "        if not meta_tag_content:\n",
    "            meta_tag_content = parser.findallbyattribute(attribute_name='name', attribute_value='og:title')\n",
    "        title_text_meta =parser.textbytag(meta_tag_content[0]['tag'])[0] if meta_tag_content else ''\n",
    "\n",
    "        # Further filtering of unwanted characters\n",
    "        # Alphanumeric characters, punctuation, and alphanumeric\n",
    "        filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "        filter_title_text = filter_regex.sub('', title_text).lower()\n",
    "        filter_title_text_h1 = filter_regex.sub('', title_text_h1).lower()\n",
    "        filter_title_text_meta = filter_regex.sub('', title_text_meta).lower()\n",
    "        print(filter_title_text_meta)\n",
    "        # Case1: If both matches don't do anything\n",
    "        if title_text_h1 == title_text:\n",
    "            used_delimeter = True\n",
    "        # Case2: h1 and meta tag matches(either of h1 or meta)\n",
    "        elif filter_title_text_h1 and filter_title_text_h1 == filter_title_text_meta:\n",
    "            title_text = title_text_h1\n",
    "            used_delimeter = True\n",
    "        # Case3: If both h1 and meta are substrings of title_text(use h1 if h1 is longer)\n",
    "        elif filter_title_text_h1 and filter_title_text_h1 in filter_title_text and filter_title_text_meta is not '' and filter_title_text_meta in filter_title_text and len(title_text_h1) > len(title_text_meta):\n",
    "            used_delimeter = True\n",
    "        # Case4: If title_text starts with meta text(replace with meta)e\n",
    "        elif filter_title_text_meta and filter_title_text_meta != filter_title_text and filter_title_text.startswith(filter_title_text_meta):\n",
    "            title_text = title_text_meta\n",
    "            used_delimeter = True\n",
    "        # If none of the above conditions is matched, means a delimiter must be present between them\n",
    "        # Now individually parts separated by delimiter have to be checked, and now we check with h1 tag only (Observation-based)\n",
    "        if not used_delimeter and '|' in title_text:\n",
    "            title_text = self.split_title(title_text, '|', title_text_h1)\n",
    "            used_delimeter = True\n",
    "\n",
    "        # Split title with -\n",
    "        if not used_delimeter and '-' in title_text:\n",
    "            title_text = self.split_title(title_text, '-', title_text_h1)\n",
    "            used_delimeter = True\n",
    "\n",
    "        # Split title with _\n",
    "        if not used_delimeter and '_' in title_text:\n",
    "            title_text = self.split_title(title_text, '_', title_text_h1)\n",
    "            used_delimeter = True\n",
    "\n",
    "        # Split title with /\n",
    "        if not used_delimeter and '/' in title_text:\n",
    "            title_text = self.split_title(title_text, '/', title_text_h1)\n",
    "            used_delimeter = True\n",
    "\n",
    "        # Split title with »\n",
    "        if not used_delimeter and ' » ' in title_text:\n",
    "            title_text = self.split_title(title_text, ' » ', title_text_h1)\n",
    "            used_delimeter = True\n",
    "\n",
    "        return title_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDt5gevywMRA",
    "outputId": "2ee5c5f8-a1e9-42ce-dcba-260327dbf757",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'timesofindia.indiatimes.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "title  is/are\n",
      "\n",
      "Assam CM Himanta Biswa Sarma vows to end child marriage in the state \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "url = \"https://timesofindia.indiatimes.com/city/guwahati/assam-cm-himanta-biswa-sarma-vows-to-end-child-marriage-in-the-state/articleshow/108025651.cms\"\n",
    "proxies = {\n",
    "\"http\": \"http://scraperapi:8355bf750256f87924cb321115d06996@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url,headers=headers,proxies=proxies,verify=False)\n",
    "API_KEY = \"8355bf750256f87924cb321115d06996\"\n",
    "params = {'api_key': API_KEY, 'url': url}\n",
    "response = requests.get('http://api.scraperapi.com/', params=urlencode(params))\n",
    "response.raise_for_status()\n",
    "\n",
    "html_content = response.text\n",
    "soup =HTMLParser(html_content)\n",
    "\n",
    "\n",
    "\n",
    "# print(soup)\n",
    "# Create an instance of CustomExtractor\n",
    "extractor = CustomExtractor()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "title_list=extractor.get_title(soup);\n",
    "print(\"title  is/are\")\n",
    "\n",
    "print(extractor.get_title(soup))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: import-ipynb in /home/joyjeet031/.local/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: IPython in /usr/lib/python3/dist-packages (from import-ipynb) (7.31.1)\n",
      "Requirement already satisfied: nbformat in /usr/lib/python3/dist-packages (from import-ipynb) (5.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install import-ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLHXth5VCnoBeFhz4tcqmD",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
