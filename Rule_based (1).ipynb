{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h49BdxPUCJ4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3586\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "f= open(r'stopwords_en.txt')\n",
    "fh = f.read()\n",
    "stop_words = []\n",
    "for word in fh:\n",
    "  stop_words.append(word)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t41VJtJaHdf-"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class CustomExtractor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stop_words)\n",
    "\n",
    "    def calculate_gravity_score(self, tag):\n",
    "        # Your custom logic to calculate the gravity score\n",
    "        # This example uses the length of text content as a score\n",
    "        text_content = tag.get_text(strip=True)\n",
    "        return len(text_content)\n",
    "\n",
    "    def walk_siblings(self, node):\n",
    "        # Iterate over siblings\n",
    "        siblings = []\n",
    "        for sibling in node.find_all_next():\n",
    "            if sibling.name and sibling.name != 'text':\n",
    "                siblings.append(sibling)\n",
    "        return siblings\n",
    "\n",
    "    def is_highlink_density(self, node):\n",
    "\n",
    "        # For simplicity, this example checks if the node contains more than 5 links\n",
    "        links = node.find_all('a')\n",
    "        return len(links) > 5\n",
    "\n",
    "    def is_boostable(self, node):\n",
    "        para = \"p\"\n",
    "        steps_away = 0\n",
    "        minimum_stopword_count = 5\n",
    "        max_stepsaway_from_node = 3\n",
    "\n",
    "        nodes = self.walk_siblings(node)\n",
    "        for current_node in nodes:\n",
    "            current_node_tag = current_node.name\n",
    "            if current_node_tag == para:\n",
    "                if steps_away >= max_stepsaway_from_node:\n",
    "                    return False\n",
    "                paragraph_text = current_node.get_text(strip=True)\n",
    "                word_stats = self.get_stopword_count(paragraph_text)\n",
    "                if word_stats > minimum_stopword_count:\n",
    "                    return True\n",
    "                steps_away += 1\n",
    "        return False\n",
    "\n",
    "    def get_stopword_count(self, text):\n",
    "        words = [word.lower() for word in text.split()]\n",
    "        sm = 0\n",
    "        for word in words:\n",
    "          if(word in self.stopwords):\n",
    "            sm+=1\n",
    "        return sm\n",
    "\n",
    "\n",
    "    def calculate_best_node(self, soup):\n",
    "        top_node = None\n",
    "        top_node_score = 0\n",
    "        nodes_to_check = self.nodes_to_check(soup)\n",
    "        # print(nodes_to_check)\n",
    "        starting_boost = 1.0\n",
    "        cnt = 0\n",
    "        i = 0\n",
    "        parent_nodes = []\n",
    "        nodes_with_text = []\n",
    "\n",
    "\n",
    "        for node in nodes_to_check:\n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "\n",
    "            high_link_density = self.is_highlink_density(node)\n",
    "            if word_stats >= 2 and not high_link_density:\n",
    "                nodes_with_text.append(node)\n",
    "\n",
    "        nodes_number = len(nodes_with_text)\n",
    "        negative_scoring = 0\n",
    "        bottom_negativescore_nodes = nodes_number * 0.25\n",
    "\n",
    "\n",
    "        for node in nodes_with_text:\n",
    "\n",
    "            boost_score = 0.0\n",
    "            if self.is_boostable(node):\n",
    "                if cnt >= 0:\n",
    "                    boost_score = 1.0 / starting_boost * 50\n",
    "                    starting_boost += 1\n",
    "\n",
    "            if nodes_number > 15:\n",
    "                if (nodes_number - i) <= bottom_negativescore_nodes:\n",
    "                    booster = bottom_negativescore_nodes - (nodes_number - i)\n",
    "                    boost_score = -pow(booster, 2)\n",
    "                    negscore = abs(boost_score) + negative_scoring\n",
    "                    if negscore > 40:\n",
    "                        boost_score = 5.0\n",
    "\n",
    "\n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "            upscore = word_stats + boost_score\n",
    "\n",
    "            parent_node = node.parent\n",
    "            self.update_score(parent_node, upscore)\n",
    "            self.update_node_count(parent_node, 1)\n",
    "\n",
    "            if parent_node not in parent_nodes:\n",
    "                parent_nodes.append(parent_node)\n",
    "\n",
    "            parent_parent_node = parent_node.parent\n",
    "            if parent_parent_node is not None:\n",
    "                self.update_node_count(parent_parent_node, 1)\n",
    "                self.update_score(parent_parent_node, upscore / 2)\n",
    "                if parent_parent_node not in parent_nodes:\n",
    "                    parent_nodes.append(parent_parent_node)\n",
    "            cnt += 1\n",
    "            i += 1\n",
    "\n",
    "        for e in parent_nodes:\n",
    "            score = self.get_score(e)\n",
    "\n",
    "            if score > top_node_score:\n",
    "                top_node = e\n",
    "                top_node_score = score\n",
    "\n",
    "            if top_node is None:\n",
    "                top_node = e\n",
    "\n",
    "        return top_node\n",
    "\n",
    "    def nodes_to_check(self, soup):\n",
    "        nodes_to_check = []\n",
    "        for tag in ['p', 'pre', 'td','div']:\n",
    "            items = soup.find_all(tag)\n",
    "            nodes_to_check += items\n",
    "        return nodes_to_check\n",
    "\n",
    "    def update_score(self, node, score):\n",
    "        if 'score' not in node:\n",
    "            node['score'] = len(node.get_text(strip=True))\n",
    "        node['score'] += score\n",
    "\n",
    "    def update_node_count(self, node, count):\n",
    "        if 'count' not in node:\n",
    "            node['count'] = 0\n",
    "        node['count'] += count\n",
    "\n",
    "    def get_score(self, node):\n",
    "        return node.get('score', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDt5gevywMRA",
    "outputId": "2ee5c5f8-a1e9-42ce-dcba-260327dbf757"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomExtractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21591/4172533500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# print(soup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Create an instance of CustomExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Calculate best node based on custom gravity scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomExtractor' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "url =\"https://www.scmp.com/\"\n",
    "# proxies = {\n",
    "# \"http\": \"http://scraperapi:8355bf750256f87924cb321115d06996@proxy-server.scraperapi.com:8001\"\n",
    "# }\n",
    "# headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "# response = requests.get(url,headers=headers,proxies=proxies,verify=False)\n",
    "API_KEY = \"8355bf750256f87924cb321115d06996\"\n",
    "params = {'api_key': API_KEY, 'url': url}\n",
    "response = requests.get('http://api.scraperapi.com/', params=urlencode(params))\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# print(soup)\n",
    "# Create an instance of CustomExtractor\n",
    "extractor = CustomExtractor()\n",
    "\n",
    "# Calculate best node based on custom gravity scores\n",
    "best_node = extractor.calculate_best_node(soup)\n",
    "\n",
    "# Now, you can access the best node and its gravity score\n",
    "if best_node:\n",
    "    print(f\"Best Node: {best_node['class']}, Gravity Score: {best_node.get('score', 0)}\")\n",
    "    print(best_node.get_text(strip=True))\n",
    "else:\n",
    "    print(\"No best node found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-muHJzo3i_l",
    "outputId": "bd137e9e-3732-4a17-8242-2d77e63477ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "qk9pvfb08V3G"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_title(soup):\n",
    "    \"\"\"Explicit rules:\n",
    "    1. title == h1, no need to split\n",
    "    2. h1 similar to og:title, use h1\n",
    "    3. title contains h1, title contains og:title, len(h1) > len(og:title), use h1\n",
    "    4. title starts with og:title, use og:title\n",
    "    5. use title, after splitting\n",
    "    \"\"\"\n",
    "    title = ''\n",
    "    title_element = soup.title\n",
    "\n",
    "    # no title found\n",
    "    if title_element is None or len(title_element) == 0:\n",
    "        print(\"Error\")\n",
    "        return title\n",
    "\n",
    "    # title elem found\n",
    "    title_text = title_element.text\n",
    "    used_delimeter = False\n",
    "\n",
    "#     title from h1\n",
    "    # - extract the longest text from all h1 elements\n",
    "    # - too short texts (fewer than 2 words) are discarded\n",
    "    # - clean double spaces\n",
    "#     h1_element = soup.find_all('h1')[0]\n",
    "#     title_text_h1 = h1_element.text\n",
    "    title_text_h1=''\n",
    "    title_element_h1_list = soup.find_all('h1')\n",
    "    title_text_h1_list = [tag.get_text(strip=True) for tag in title_element_h1_list]\n",
    "    if title_text_h1_list:\n",
    "        title_text_h1_list.sort(key=len, reverse=True)\n",
    "        #longest title\n",
    "        title_text_h1 = title_text_h1_list[0]\n",
    "        # clean double spaces\n",
    "        title_text_h1 = ' '.join([x for x in title_text_h1.split() if x])\n",
    "    #title from meta tag(not user-visible)\n",
    "    meta_tag_content = soup.find({'meta': {'property': 'og:title'}})\n",
    "    if not meta_tag_content:\n",
    "        meta_tag_content = soup.find({'meta': {'name': 'og:title'}})\n",
    "    title_text_meta = meta_tag_content.get('content', '')  # Empty string if no meta tag found\n",
    "    # Further filtering of unwanted characters\n",
    "    # Alphanumeric characters, punctuation and alphanumeric\n",
    "    filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "    filter_title_text = filter_regex.sub('', title_text).lower()\n",
    "    filter_title_text_h1 = filter_regex.sub('', title_text_h1).lower()\n",
    "    filter_title_text_meta = filter_regex.sub('', title_text_meta).lower()\n",
    "    \n",
    "    # Case1: If both matches don't do anything\n",
    "    if title_text_h1 == title_text:\n",
    "        used_delimeter = True\n",
    "    # Case2: h1 and meta tag matches(either of h1 or meta)\n",
    "    elif filter_title_text_h1 and filter_title_text_h1 == filter_title_text_meta:\n",
    "        title_text = title_text_h1\n",
    "        used_delimeter = True\n",
    "    # Case3: If both h1 and meta are a substring of title_text(use h1)\n",
    "    elif filter_title_text_h1 and filter_title_text_h1 in filter_title_text and filter_title_text_meta in filter_title_text  and len(title_text_h1) > len(title_text_meta):\n",
    "        title_text = title_text_h1\n",
    "        used_delimeter = True\n",
    "    # Case4: If title_text startswith meta text(replace with meta)\n",
    "    elif filter_title_text_meta and filter_title_text_meta != filter_title_text and filter_title_text.startswith(filter_title_text_meta):\n",
    "        title_text = title_text_meta\n",
    "        used_delimeter = True\n",
    "    \n",
    "    # If none of the above condition is matched, means a delimiter must be present between them\n",
    "    # Now individually parts separated by delimiter has to be checked and now we check with h1 tag only(no meta tag)-Observation based\n",
    "    if not used_delimeter and '|' in title_text:\n",
    "        title_text = split_title(title_text, '|', title_text_h1)\n",
    "        used_delimeter = True\n",
    "\n",
    "    # split title with -\n",
    "    if not used_delimeter and '-' in title_text:\n",
    "        title_text = split_title(title_text, '-', title_text_h1)\n",
    "        used_delimeter = True\n",
    "\n",
    "    # split title with _\n",
    "    if not used_delimeter and '_' in title_text:\n",
    "        title_text = split_title(title_text, '_', title_text_h1)\n",
    "        used_delimeter = True\n",
    "\n",
    "    # split title with /\n",
    "    if not used_delimeter and '/' in title_text:\n",
    "        title_text = split_title(title_text, '/', title_text_h1)\n",
    "        used_delimeter = True\n",
    "\n",
    "    # split title with »\n",
    "    if not used_delimeter and ' » ' in title_text:\n",
    "        title_text = split_title(title_text, ' » ', title_text_h1)\n",
    "        used_delimeter = True\n",
    "    return title_text\n",
    "    \n",
    "def split_title(title, splitter, hint=None):\n",
    "    \"\"\"Split the title to best part possible\"\"\"\n",
    "    large_text_length = 0\n",
    "    large_text_index = 0\n",
    "    title_pieces = title.split(splitter)\n",
    "    if hint and hint!='':\n",
    "        filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "        hint = filter_regex.sub('', hint).lower()\n",
    "\n",
    "    # find the largest title piece\n",
    "    for i, title_piece in enumerate(title_pieces):\n",
    "        current = title_piece.strip()\n",
    "        #Immediately break if any part matches\n",
    "        if hint and hint in filter_regex.sub('', current).lower():\n",
    "            large_text_index = i\n",
    "            break\n",
    "        if len(current) > large_text_length:\n",
    "            large_text_length = len(current)\n",
    "            large_text_index = i\n",
    "\n",
    "#     Even if no part matches with hint(h1) if prints simply the longest part as the parts\n",
    "#     are usually of independent meaning\n",
    "    title = title_pieces[large_text_index]\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking China, Asia, HK News, Opinions and Insights \n"
     ]
    }
   ],
   "source": [
    "print(get_title(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLHXth5VCnoBeFhz4tcqmD",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
