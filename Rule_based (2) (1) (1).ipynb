{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h49BdxPUCJ4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3585\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "f= open(r\"C:\\Users\\Prakhar\\OneDrive\\Desktop\\stop_words_en.txt\")\n",
    "fh = f.read()\n",
    "stop_words = []\n",
    "for word in fh:\n",
    "  stop_words.append(word)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t41VJtJaHdf-"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as date_parser\n",
    "\n",
    "\n",
    "class CustomExtractor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = stop_words\n",
    "\n",
    "    def calculate_gravity_score(self, tag):\n",
    "        # Your custom logic to calculate the gravity score\n",
    "        # This example uses the length of text content as a score\n",
    "        text_content = tag.get_text(strip=True)\n",
    "        return len(text_content)\n",
    "\n",
    "    def walk_siblings(self, node):\n",
    "        # Iterate over siblings\n",
    "        siblings = []\n",
    "        for sibling in node.find_all_next():\n",
    "            if sibling.name:\n",
    "                siblings.append(sibling)\n",
    "        return siblings\n",
    "\n",
    "    def is_highlink_density(self, node):\n",
    "\n",
    "        # For simplicity, this example checks if the node contains more than 5 links\n",
    "        links = node.find_all('a')\n",
    "        return len(links) > 25\n",
    "\n",
    "    def is_boostable(self, node):\n",
    "        para = \"p\"\n",
    "        steps_away = 0\n",
    "        minimum_stopword_count = 5\n",
    "        max_stepsaway_from_node = 3\n",
    "\n",
    "        nodes = self.walk_siblings(node)\n",
    "        for current_node in nodes:\n",
    "            current_node_tag = current_node.name\n",
    "            if current_node_tag == para:\n",
    "                if steps_away >= max_stepsaway_from_node:\n",
    "                    return False\n",
    "                paragraph_text = current_node.get_text(strip=True)\n",
    "                word_stats = self.get_stopword_count(paragraph_text)\n",
    "                if word_stats > minimum_stopword_count:\n",
    "                    return True\n",
    "                steps_away += 1\n",
    "        return False\n",
    "\n",
    "    def get_stopword_count(self, text):\n",
    "        words = [word.lower() for word in text.split()]\n",
    "        sm = 0\n",
    "        for word in words:\n",
    "          if(word in self.stopwords):\n",
    "            sm+=1\n",
    "        return sm\n",
    "\n",
    "\n",
    "    def calculate_best_node(self, soup):\n",
    "        top_node = None\n",
    "        top_node_score = 0\n",
    "        nodes_to_check = self.nodes_to_check(soup)\n",
    "        # print(nodes_to_check)\n",
    "        starting_boost = 1.0\n",
    "        cnt = 0\n",
    "        i = 0\n",
    "        parent_nodes = []\n",
    "        nodes_with_text = []\n",
    "\n",
    "\n",
    "        for node in nodes_to_check:\n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "#             print(node.get_text(strip=True))\n",
    "            high_link_density = self.is_highlink_density(node)\n",
    "            if word_stats >= 2 and not high_link_density:\n",
    "                nodes_with_text.append(node)\n",
    "\n",
    "        nodes_number = len(nodes_with_text)\n",
    "        negative_scoring = 0\n",
    "        bottom_negativescore_nodes = nodes_number * 0.25\n",
    "\n",
    "\n",
    "        for node in nodes_with_text:\n",
    "\n",
    "            boost_score = len(node.find_all('p'))\n",
    "            \n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "            temp = 0\n",
    "            try:\n",
    "                temp = len(node.content)\n",
    "            except:\n",
    "                pass\n",
    "            upscore = word_stats + boost_score - abs((temp - 3*boost_score))**0.25\n",
    "            \n",
    "#             if(node.has_attr('class')):\n",
    "#                 print(node['class'])\n",
    "#             print(node.name,\" \",word_stats,\" \",boost_score)\n",
    "            \n",
    "            parent_node = node.parent\n",
    "            self.update_score(node,upscore)\n",
    "            \n",
    "            x = node\n",
    "            if(node.name == 'p'):\n",
    "                for i in range(5):\n",
    "                    x = x.parent\n",
    "                    self.update_score(x,upscore*((0.8)**i))\n",
    "\n",
    "            if parent_node not in parent_nodes:\n",
    "                parent_nodes.append(parent_node)\n",
    "            \n",
    "            parent_parent_node = parent_node.parent\n",
    "            if parent_parent_node is not None:\n",
    "                self.update_node_count(parent_parent_node, 1)\n",
    "        \n",
    "                if parent_parent_node not in parent_nodes:\n",
    "                    parent_nodes.append(parent_parent_node)\n",
    "            cnt += 1\n",
    "            i += 1\n",
    "            \n",
    "\n",
    "        for e in parent_nodes:\n",
    "            score = self.get_score(e)\n",
    "\n",
    "            if score > top_node_score:\n",
    "#                 print(node.name,\" \",node['score'])\n",
    "                top_node = e\n",
    "                top_node_score = score\n",
    "\n",
    "            if top_node is None:\n",
    "                top_node = e\n",
    "\n",
    "        return top_node\n",
    "\n",
    "    def nodes_to_check(self, soup):\n",
    "        nodes_to_check = []\n",
    "        for tag in ['div','p', 'pre', 'td']:\n",
    "            items = soup.find_all(tag)\n",
    "            nodes_to_check += items\n",
    "#         print(nodes_to_check)\n",
    "        return nodes_to_check\n",
    "    \n",
    "\n",
    "    def update_score(self, node, score):\n",
    "        if 'score' not in node:\n",
    "            node['score'] = len(node.get_text(strip=True))**(0.5)\n",
    "        node['score'] += score\n",
    "\n",
    "    def update_node_count(self, node, count):\n",
    "        if 'count' not in node:\n",
    "            node['count'] = 0\n",
    "        \n",
    "        node['count'] += count\n",
    "\n",
    "    def get_score(self, node):\n",
    "        return node.get('score', 0)\n",
    " \n",
    "    def get_authors(self, doc):\n",
    "        def contains_digits(d):\n",
    "            for char in d:\n",
    "                if char.isdigit():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        def unique_list(lst):\n",
    "            count = {}\n",
    "            list = []\n",
    "            for item in lst:\n",
    "                if item.lower() in count:\n",
    "                    continue\n",
    "                count[item.lower()] = 1\n",
    "                list.append(item.title())\n",
    "            return list\n",
    "    \n",
    "        def parse(str):\n",
    "            \n",
    "            str = ''.join(char for char in str if char != '<' and char != '>')\n",
    "\n",
    "            str = str.replace('By:', '').replace('From:', '').strip()\n",
    "            \n",
    "            names = [s.strip() for s in re.split(r\"[^\\w\\'\\-\\.]\", str) if s]\n",
    "\n",
    "    \n",
    "            authors = []\n",
    "            current = []\n",
    "            delimiters = ['and', ',', '']\n",
    "    \n",
    "            for name in names:\n",
    "                if name in delimiters:\n",
    "                    if len(current) > 0:\n",
    "                        authors.append(' '.join(current))\n",
    "                        current = []\n",
    "                elif not contains_digits(name):\n",
    "                    current.append(name)\n",
    "    \n",
    "            valid_name = (len(current) >= 2)\n",
    "            if valid_name:\n",
    "                authors.append(' '.join(current))\n",
    "    \n",
    "            return authors\n",
    "        patterns=[re.compile(r'(?:author.*?name|name.*?author)', re.IGNORECASE)]\n",
    "        attributes = ['name', 'rel', 'itemprop', 'class', 'id']\n",
    "        variables = ['author', 'byline', 'dc.creator', 'byl','group-info']\n",
    "        matches = []\n",
    "        authors = []\n",
    "    \n",
    "        for attr in attributes:\n",
    "            for val in variables+patterns:\n",
    "                found = doc.find_all(attrs={attr: val})\n",
    "                matches.extend(found)\n",
    "        \n",
    "\n",
    "        for match in matches:\n",
    "            content = ''\n",
    "            if match.tag == 'meta':\n",
    "                content_value = match.get('content')\n",
    "                if len(content_value) > 0:\n",
    "                    content = content_value[0]\n",
    "            else:\n",
    "                content = match.get_text() or ''\n",
    "            if len(content) > 0:\n",
    "                authors.extend(parse(content))\n",
    "    \n",
    "        return unique_list(authors)\n",
    "    \n",
    "    def publishing_date(self, url, doc):\n",
    "        def parse_date(str):\n",
    "            if str:\n",
    "                try:\n",
    "                    return date_parser(str)\n",
    "                except (ValueError, OverflowError, AttributeError, TypeError):\n",
    "                    return None\n",
    "    \n",
    "        \n",
    "        STRICT_DATE_REGEX = re.compile(r'\\/(\\d{4})\\/(\\d{2})\\/(\\d{2})\\/')\n",
    "        date_pattern = re.compile(r'(\\d{2} [a-zA-Z]{3} \\d{4}) (\\d{2}:\\d{2}[APMapm]{2})')\n",
    "        date_match = STRICT_DATE_REGEX.search(url)\n",
    "        if date_match:\n",
    "            str = date_match.group(0)\n",
    "            datetime = parse_date(str)\n",
    "            if datetime:\n",
    "                return datetime\n",
    "    \n",
    "        \n",
    "        date_tags = [\n",
    "            {'attribute': ('property', 'rnews:datePublished'), 'content': 'content'},\n",
    "            {'attribute': ('property', 'article:published_time'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'OriginalPublicationDate'), 'content': 'content'},\n",
    "            {'attribute': ('itemprop', 'datePublished'), 'content': 'datetime'},\n",
    "            {'attribute': ('property', 'og:published_time'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'article_date_original'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'publication_date'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'sailthru.date'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'PublishDate'), 'content': 'content'},\n",
    "            {'attribute': ('pubdate', 'pubdate'), 'content': 'datetime'},\n",
    "            {'attribute': ('name', 'publish_date'), 'content': 'content'},\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        patterns = re.compile(r'(?:article.*publish|publish.*article|\\bdate\\b|\\btime\\b)')\n",
    "\n",
    "        for tags in date_tags:\n",
    "            meta_tags = doc.find_all(attrs={tags['attribute'][0]: tags['attribute'][1]})\n",
    "            if meta_tags:\n",
    "                str = meta_tags[0].get(tags['content'])\n",
    "                datetime = parse_date(str)\n",
    "                if datetime:\n",
    "                    return datetime\n",
    "        \n",
    "        additional_date_tag = doc.find('div', class_=lambda c: c and patterns.search(c))\n",
    "        if additional_date_tag:\n",
    "            str = additional_date_tag.get_text(strip=True)\n",
    "            match = date_pattern.search(str)\n",
    "            if match:\n",
    "                date_str, time_str = match.groups()\n",
    "\n",
    "                # Combine date and time, then parse using dateutil.parser\n",
    "                datetime_str = f\"{date_str} {time_str}\"\n",
    "                datetime_obj = date_parser(datetime_str) \n",
    "            return datetime_obj\n",
    "        \n",
    "\n",
    "    \n",
    "        # If none of the strategies work, return None\n",
    "        return None\n",
    "    def split_title(self,title, splitter, hint=None):\n",
    "        \"\"\"Split the title to best part possible\"\"\"\n",
    "        large_text_length = 0\n",
    "        large_text_index = 0\n",
    "        title_pieces = title.split(splitter)\n",
    "        if hint and hint!='':\n",
    "            filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "            hint = filter_regex.sub('', hint).lower()\n",
    "    \n",
    "        # find the largest title piece\n",
    "        for i, title_piece in enumerate(title_pieces):\n",
    "            current = title_piece.strip()\n",
    "            #Immediately break if any part matches\n",
    "            if hint and hint in filter_regex.sub('', current).lower():\n",
    "                large_text_index = i\n",
    "                break\n",
    "            if len(current) > large_text_length:\n",
    "                large_text_length = len(current)\n",
    "                large_text_index = i\n",
    "    \n",
    "    #     Even if no part matches with hint(h1) if prints simply the longest part as the parts\n",
    "    #     are usually of independent meaning\n",
    "        title = title_pieces[large_text_index]\n",
    "        return title    \n",
    "    def get_title(self,soup):\n",
    "        \"\"\"Explicit rules:\n",
    "        1. title == h1, no need to split\n",
    "        2. h1 similar to og:title, use h1\n",
    "        3. title contains h1, title contains og:title, len(h1) > len(og:title), use h1\n",
    "        4. title starts with og:title, use og:title\n",
    "        5. use title, after splitting\n",
    "        \"\"\"\n",
    "        title = ''\n",
    "        title_element = soup.title\n",
    "    \n",
    "        # no title found\n",
    "        if title_element is None or len(title_element) == 0:\n",
    "            print(\"Error\")\n",
    "            return title\n",
    "    \n",
    "        # title elem found\n",
    "        title_text = title_element.text\n",
    "        used_delimeter = False\n",
    "    \n",
    "    #     title from h1\n",
    "            # - extract the longest text from all h1 elements\n",
    "        # - too short texts (fewer than 2 words) are discarded\n",
    "        # - clean double spaces\n",
    "    #     h1_element = soup.find_all('h1')[0]\n",
    "    #     title_text_h1 = h1_element.text\n",
    "        title_text_h1=''\n",
    "        title_element_h1_list = soup.find_all('h1')\n",
    "        title_text_h1_list = [tag.get_text(strip=True) for tag in title_element_h1_list]\n",
    "        if title_text_h1_list:\n",
    "            title_text_h1_list.sort(key=len, reverse=True)\n",
    "            #longest title\n",
    "            title_text_h1 = title_text_h1_list[0]\n",
    "            # clean double spaces\n",
    "            title_text_h1 = ' '.join([x for x in title_text_h1.split() if x])\n",
    "        #title from meta tag(not user-visible)\n",
    "        meta_tag_content = soup.find({'meta': {'property': 'og:title'}})\n",
    "        if not meta_tag_content:\n",
    "            meta_tag_content = soup.find({'meta': {'name': 'og:title'}})\n",
    "        title_text_meta = meta_tag_content.get('content', '')  # Empty string if no meta tag found\n",
    "        # Further filtering of unwanted characters\n",
    "        # Alphanumeric characters, punctuation and alphanumeric\n",
    "        filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "        filter_title_text = filter_regex.sub('', title_text).lower()\n",
    "        filter_title_text_h1 = filter_regex.sub('', title_text_h1).lower()\n",
    "        filter_title_text_meta = filter_regex.sub('', title_text_meta).lower()\n",
    "        \n",
    "        # Case1: If both matches don't do anything\n",
    "        if title_text_h1 == title_text:\n",
    "            used_delimeter = True\n",
    "        # Case2: h1 and meta tag matches(either of h1 or meta)\n",
    "        elif filter_title_text_h1 and filter_title_text_h1 == filter_title_text_meta:\n",
    "            title_text = title_text_h1\n",
    "            used_delimeter = True\n",
    "        # Case3: If both h1 and meta are a substring of title_text(use h1)\n",
    "        elif filter_title_text_h1 and filter_title_text_h1 in filter_title_text and filter_title_text_meta in filter_title_text  and len(title_text_h1) > len(title_text_meta):\n",
    "            title_text = title_text_h1\n",
    "            used_delimeter = True\n",
    "        # Case4: If title_text startswith meta text(replace with meta)\n",
    "        elif filter_title_text_meta and filter_title_text_meta != filter_title_text and filter_title_text.startswith(filter_title_text_meta):\n",
    "            title_text = title_text_meta\n",
    "            used_delimeter = True\n",
    "        \n",
    "        # If none of the above condition is matched, means a delimiter must be present between them\n",
    "        # Now individually parts separated by delimiter has to be checked and now we check with h1 tag only(no meta tag)-Observation based\n",
    "        if not used_delimeter and '|' in title_text:\n",
    "            title_text = self.split_title(title_text, '|', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with -\n",
    "        if not used_delimeter and '-' in title_text:\n",
    "            title_text = self.split_title(title_text, '-', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with _\n",
    "        if not used_delimeter and '_' in title_text:\n",
    "            title_text = self.split_title(title_text, '_', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with /\n",
    "        if not used_delimeter and '/' in title_text:\n",
    "            title_text = self.split_title(title_text, '/', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with »\n",
    "        if not used_delimeter and ' » ' in title_text:\n",
    "            title_text = self.split_title(title_text, ' » ', title_text_h1)\n",
    "            used_delimeter = True\n",
    "        return title_text\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDt5gevywMRA",
    "outputId": "2ee5c5f8-a1e9-42ce-dcba-260327dbf757"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.bangkokpost.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors  are\n",
      "No authors found.\n",
      "date of publication: None\n",
      "title  is/are\n",
      "Police nab 3 'call centre' suspects\n",
      "\n",
      "\n",
      "\n",
      "Best Node: ['divsection-container'], Gravity Score: 81.59286689661633\n",
      "Police nab 3 'call centre' suspects10ThailandGeneralPolice nab 3 'call centre' suspectsAccused 'ran operation' in SouthPUBLISHED : 28 Jan 2024 at 07:09NEWSPAPER SECTION:NewsWRITER:Wassayos Ngamkham110Pol Lt Gen Jirabhop Bhuridej, Chief of the Central Investigation Bureau, holds a press conference on the scam. (Police photo)Police arrested suspected members of a call centre gang in Sungai Kolok district in Narathiwat province on Saturday.Pol Lt Gen Jirabhop Bhuridej, Chief of the Central Investigation Bureau, said the CIB worked with the Crime Suppression Division Subdivision 6 and local police to raid a house in the district, which borders with Malaysia.The suspects were identified as Sarina, a 24-year-old local woman, and Malaysians Kiang Wan, 25, and Kuok Rong, 36.Police also seized 15 Sim boxes and GSM gateways and three WiFi routers during the raid.Pol Lt Gen Jirabhop said one Sim box, priced at about 100,000 baht, can call 480 potential victims at one time.The suspects are accused of using unauthorised radio communications equipment and running an illegal telecommunication business.One suspect, Mr Wan, is also accused of illegal entry to Thailand, according to Pol Lt Gen Jirabhop.Police said Ms Sarina and Mr Wan, accepted the charges, saying they worked as house caretakers under a Malaysian employer, and used their salaries to rent a house and buy the equipment.Mr Rong, however, denied the charges, saying that he merely transferred money to Ms Sarina without involving himself in the scam, police said.Police will expand the investigation into the gang, said Pol Lt Gen Jirabhop.The raid in Narathiwat followed an investigation into unauthorised GSM gateway installation for call centre scams in the province, he said.This particular gang used a Sim box technique to divert international calls to a cellular device via the internet.To do so, he said the gang had used Sim Bank, hardware capable of reading a bank for Sim cards, to communicate with GSM gateways installed in other places via a cloud server.The gateways then transferred the internet signal into a cellular signal, as indicated by the local country code displayed on the victim's mobile screen, he said.Doing so can help the scammer get away with violating the National Broadcasting and Telecommunication Commission's (NBTC) regulation on telephone prefixes, he said.Both Sim Bank and GSM gateways are expensive tools that need NBTC authorisation before they can be used, he said.KEYWORDSPhoneScamSIM cardFraudCrimeDo you like the content of this article?COMMENT(10)RECOMMENDEDPM sceptical of opinion poll on digital handoutT3 Technology becomes Tuya Cube developer, Enhancing “Own the Household” Smart SolutionsGovt sees soft power boost in festival hitsPWA, NPNL and SPEB Elevate Waterworks HR StandardsKingdom promotes ties with the PhilippinesTRENDING\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "url = \"https://www.bangkokpost.com/thailand/general/2731695/police-nab-3-call-centre-suspects?tbref=hp\"\n",
    "proxies = {\n",
    "\"http\": \"http://scraperapi:8355bf750256f87924cb321115d06996@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url,headers=headers,proxies=proxies,verify=False)\n",
    "API_KEY = \"8355bf750256f87924cb321115d06996\"\n",
    "params = {'api_key': API_KEY, 'url': url}\n",
    "response = requests.get('http://api.scraperapi.com/', params=urlencode(params))\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# print(soup)\n",
    "# Create an instance of CustomExtractor\n",
    "extractor = CustomExtractor()\n",
    "\n",
    "\n",
    "authors_list=extractor.get_authors(soup);\n",
    "print(\"authors  are\")\n",
    "\n",
    "if authors_list:\n",
    "    \n",
    "    for author in authors_list:\n",
    "        print(author)\n",
    "       \n",
    "else:\n",
    "    print(\"No authors found.\")\n",
    "date=extractor.publishing_date(url,soup)\n",
    "\n",
    "print(\"date of publication:\",date)\n",
    "\n",
    "title_list=extractor.get_title(soup);\n",
    "print(\"title  is/are\")\n",
    "\n",
    "print(extractor.get_title(soup))\n",
    "\n",
    "\n",
    "# Calculate best node based on custom gravity scores\n",
    "best_node = extractor.calculate_best_node(soup)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "# Now, you can access the best node and its gravity score\n",
    "if best_node:\n",
    "    print(f\"Best Node: {best_node['class']}, Gravity Score: {best_node.get('score', 0)}\")\n",
    "    print(best_node.get_text(strip=True))\n",
    "else:\n",
    "    print(\"No best node found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLHXth5VCnoBeFhz4tcqmD",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
