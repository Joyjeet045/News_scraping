{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "h49BdxPUCJ4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3586\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "f= open(r'stopwords_en.txt')\n",
    "fh = f.read()\n",
    "stop_words = []\n",
    "for word in fh:\n",
    "  stop_words.append(word)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "t41VJtJaHdf-"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as date_parser\n",
    "\n",
    "\n",
    "class CustomExtractor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stop_words)\n",
    "\n",
    "    def calculate_gravity_score(self, tag):\n",
    "        # Your custom logic to calculate the gravity score\n",
    "        # This example uses the length of text content as a score\n",
    "        text_content = tag.get_text(strip=True)\n",
    "        return len(text_content)\n",
    "\n",
    "    def walk_siblings(self, node):\n",
    "        # Iterate over siblings\n",
    "        siblings = []\n",
    "        for sibling in node.find_all_next():\n",
    "            if sibling.name and sibling.name != 'text':\n",
    "                siblings.append(sibling)\n",
    "        return siblings\n",
    "\n",
    "    def is_highlink_density(self, node):\n",
    "\n",
    "        # For simplicity, this example checks if the node contains more than 5 links\n",
    "        links = node.find_all('a')\n",
    "        return len(links) > 5\n",
    "\n",
    "    def is_boostable(self, node):\n",
    "        para = \"p\"\n",
    "        steps_away = 0\n",
    "        minimum_stopword_count = 5\n",
    "        max_stepsaway_from_node = 3\n",
    "\n",
    "        nodes = self.walk_siblings(node)\n",
    "        for current_node in nodes:\n",
    "            current_node_tag = current_node.name\n",
    "            if current_node_tag == para:\n",
    "                if steps_away >= max_stepsaway_from_node:\n",
    "                    return False\n",
    "                paragraph_text = current_node.get_text(strip=True)\n",
    "                word_stats = self.get_stopword_count(paragraph_text)\n",
    "                if word_stats > minimum_stopword_count:\n",
    "                    return True\n",
    "                steps_away += 1\n",
    "        return False\n",
    "\n",
    "    def get_stopword_count(self, text):\n",
    "        words = [word.lower() for word in text.split()]\n",
    "        sm = 0\n",
    "        for word in words:\n",
    "          if(word in self.stopwords):\n",
    "            sm+=1\n",
    "        return sm\n",
    "\n",
    "\n",
    "    def calculate_best_node(self, soup):\n",
    "        top_node = None\n",
    "        top_node_score = 0\n",
    "        nodes_to_check = self.nodes_to_check(soup)\n",
    "        # print(nodes_to_check)\n",
    "        starting_boost = 1.0\n",
    "        cnt = 0\n",
    "        i = 0\n",
    "        parent_nodes = []\n",
    "        nodes_with_text = []\n",
    "\n",
    "\n",
    "        for node in nodes_to_check:\n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "\n",
    "            high_link_density = self.is_highlink_density(node)\n",
    "            if word_stats >= 2 and not high_link_density:\n",
    "                nodes_with_text.append(node)\n",
    "\n",
    "        nodes_number = len(nodes_with_text)\n",
    "        negative_scoring = 0\n",
    "        bottom_negativescore_nodes = nodes_number * 0.25\n",
    "\n",
    "\n",
    "        for node in nodes_with_text:\n",
    "\n",
    "            boost_score = 0.0\n",
    "            if self.is_boostable(node):\n",
    "                if cnt >= 0:\n",
    "                    boost_score = 1.0 / starting_boost * 50\n",
    "                    starting_boost += 1\n",
    "\n",
    "            if nodes_number > 15:\n",
    "                if (nodes_number - i) <= bottom_negativescore_nodes:\n",
    "                    booster = bottom_negativescore_nodes - (nodes_number - i)\n",
    "                    boost_score = -pow(booster, 2)\n",
    "                    negscore = abs(boost_score) + negative_scoring\n",
    "                    if negscore > 40:\n",
    "                        boost_score = 5.0\n",
    "\n",
    "\n",
    "            text_node = node.get_text(strip=True)\n",
    "            word_stats = self.get_stopword_count(text_node)\n",
    "            upscore = word_stats + boost_score\n",
    "\n",
    "            parent_node = node.parent\n",
    "            self.update_score(parent_node, upscore)\n",
    "            self.update_node_count(parent_node, 1)\n",
    "\n",
    "            if parent_node not in parent_nodes:\n",
    "                parent_nodes.append(parent_node)\n",
    "\n",
    "            parent_parent_node = parent_node.parent\n",
    "            if parent_parent_node is not None:\n",
    "                self.update_node_count(parent_parent_node, 1)\n",
    "                self.update_score(parent_parent_node, upscore / 2)\n",
    "                if parent_parent_node not in parent_nodes:\n",
    "                    parent_nodes.append(parent_parent_node)\n",
    "            cnt += 1\n",
    "            i += 1\n",
    "\n",
    "        for e in parent_nodes:\n",
    "            score = self.get_score(e)\n",
    "\n",
    "            if score > top_node_score:\n",
    "                top_node = e\n",
    "                top_node_score = score\n",
    "\n",
    "            if top_node is None:\n",
    "                top_node = e\n",
    "\n",
    "        return top_node\n",
    "\n",
    "    def nodes_to_check(self, soup):\n",
    "        nodes_to_check = []\n",
    "        for tag in ['p', 'pre', 'td','div']:\n",
    "            items = soup.find_all(tag)\n",
    "            nodes_to_check += items\n",
    "        return nodes_to_check\n",
    "\n",
    "    def update_score(self, node, score):\n",
    "        if 'score' not in node:\n",
    "            node['score'] = len(node.get_text(strip=True))\n",
    "        node['score'] += score\n",
    "\n",
    "    def update_node_count(self, node, count):\n",
    "        if 'count' not in node:\n",
    "            node['count'] = 0\n",
    "        node['count'] += count\n",
    "\n",
    "    def get_score(self, node):\n",
    "        return node.get('score', 0)\n",
    " \n",
    "    def get_authors(self, doc):\n",
    "        def contains_digits(d):\n",
    "            for char in d:\n",
    "                if char.isdigit():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        def unique_list(lst):\n",
    "            count = {}\n",
    "            list = []\n",
    "            for item in lst:\n",
    "                if item.lower() in count:\n",
    "                    continue\n",
    "                count[item.lower()] = 1\n",
    "                list.append(item.title())\n",
    "            return list\n",
    "    \n",
    "        def parse(str):\n",
    "            \n",
    "            str = ''.join(char for char in str if char != '<' and char != '>')\n",
    "\n",
    "            str = str.replace('By:', '').replace('From:', '').strip()\n",
    "            \n",
    "            names = [s.strip() for s in re.split(r\"[^\\w\\'\\-\\.]\", str) if s]\n",
    "\n",
    "    \n",
    "            authors = []\n",
    "            current = []\n",
    "            delimiters = ['and', ',', '']\n",
    "    \n",
    "            for name in names:\n",
    "                if name in delimiters:\n",
    "                    if len(current) > 0:\n",
    "                        authors.append(' '.join(current))\n",
    "                        current = []\n",
    "                elif not contains_digits(name):\n",
    "                    current.append(name)\n",
    "    \n",
    "            valid_name = (len(current) >= 2)\n",
    "            if valid_name:\n",
    "                authors.append(' '.join(current))\n",
    "    \n",
    "            return authors\n",
    "        patterns=[re.compile(r'(?:author.*?name|name.*?author)', re.IGNORECASE)]\n",
    "        attributes = ['name', 'rel', 'itemprop', 'class', 'id']\n",
    "        variables = ['author', 'byline', 'dc.creator', 'byl','group-info']\n",
    "        matches = []\n",
    "        authors = []\n",
    "    \n",
    "        for attr in attributes:\n",
    "            for val in variables+patterns:\n",
    "                found = doc.find_all(attrs={attr: val})\n",
    "                matches.extend(found)\n",
    "        \n",
    "\n",
    "        for match in matches:\n",
    "            content = ''\n",
    "            if match.tag == 'meta':\n",
    "                content_value = match.get('content')\n",
    "                if len(content_value) > 0:\n",
    "                    content = content_value[0]\n",
    "            else:\n",
    "                content = match.get_text() or ''\n",
    "            if len(content) > 0:\n",
    "                authors.extend(parse(content))\n",
    "    \n",
    "        return unique_list(authors)\n",
    "    \n",
    "    def publishing_date(self, url, doc):\n",
    "        def parse_date(str):\n",
    "            if str:\n",
    "                try:\n",
    "                    return date_parser(str)\n",
    "                except (ValueError, OverflowError, AttributeError, TypeError):\n",
    "                    return None\n",
    "    \n",
    "        \n",
    "        STRICT_DATE_REGEX = re.compile(r'\\/(\\d{4})\\/(\\d{2})\\/(\\d{2})\\/')\n",
    "        date_pattern = re.compile(r'(\\d{2} [a-zA-Z]{3} \\d{4}) (\\d{2}:\\d{2}[APMapm]{2})')\n",
    "        date_match = STRICT_DATE_REGEX.search(url)\n",
    "        if date_match:\n",
    "            str = date_match.group(0)\n",
    "            datetime = parse_date(str)\n",
    "            if datetime:\n",
    "                return datetime\n",
    "    \n",
    "        \n",
    "        date_tags = [\n",
    "            {'attribute': ('property', 'rnews:datePublished'), 'content': 'content'},\n",
    "            {'attribute': ('property', 'article:published_time'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'OriginalPublicationDate'), 'content': 'content'},\n",
    "            {'attribute': ('itemprop', 'datePublished'), 'content': 'datetime'},\n",
    "            {'attribute': ('property', 'og:published_time'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'article_date_original'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'publication_date'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'sailthru.date'), 'content': 'content'},\n",
    "            {'attribute': ('name', 'PublishDate'), 'content': 'content'},\n",
    "            {'attribute': ('pubdate', 'pubdate'), 'content': 'datetime'},\n",
    "            {'attribute': ('name', 'publish_date'), 'content': 'content'},\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        patterns = re.compile(r'(?:article.*publish|publish.*article|\\bdate\\b|\\btime\\b)')\n",
    "\n",
    "        for tags in date_tags:\n",
    "            meta_tags = doc.find_all(attrs={tags['attribute'][0]: tags['attribute'][1]})\n",
    "            if meta_tags:\n",
    "                str = meta_tags[0].get(tags['content'])\n",
    "                datetime = parse_date(str)\n",
    "                if datetime:\n",
    "                    return datetime\n",
    "        \n",
    "        additional_date_tag = doc.find('div', class_=lambda c: c and patterns.search(c))\n",
    "        if additional_date_tag:\n",
    "            str = additional_date_tag.get_text(strip=True)\n",
    "            match = date_pattern.search(str)\n",
    "            if match:\n",
    "                date_str, time_str = match.groups()\n",
    "\n",
    "                # Combine date and time, then parse using dateutil.parser\n",
    "                datetime_str = f\"{date_str} {time_str}\"\n",
    "                datetime_obj = date_parser(datetime_str) \n",
    "            return datetime_obj\n",
    "        \n",
    "\n",
    "    \n",
    "        # If none of the strategies work, return None\n",
    "        return None\n",
    "    def split_title(self,title, splitter, hint=None):\n",
    "        \"\"\"Split the title to best part possible\"\"\"\n",
    "        large_text_length = 0\n",
    "        large_text_index = 0\n",
    "        title_pieces = title.split(splitter)\n",
    "        if hint and hint!='':\n",
    "            filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "            hint = filter_regex.sub('', hint).lower()\n",
    "    \n",
    "        # find the largest title piece\n",
    "        for i, title_piece in enumerate(title_pieces):\n",
    "            current = title_piece.strip()\n",
    "            #Immediately break if any part matches\n",
    "            if hint and hint in filter_regex.sub('', current).lower():\n",
    "                large_text_index = i\n",
    "                break\n",
    "            if len(current) > large_text_length:\n",
    "                large_text_length = len(current)\n",
    "                large_text_index = i\n",
    "    \n",
    "    #     Even if no part matches with hint(h1) if prints simply the longest part as the parts\n",
    "    #     are usually of independent meaning\n",
    "        title = title_pieces[large_text_index]\n",
    "        return title    \n",
    "    def get_title(self,soup):\n",
    "        \"\"\"Explicit rules:\n",
    "        1. title == h1, no need to split\n",
    "        2. h1 similar to og:title, use h1\n",
    "        3. title contains h1, title contains og:title, len(h1) > len(og:title), use h1\n",
    "        4. title starts with og:title, use og:title\n",
    "        5. use title, after splitting\n",
    "        \"\"\"\n",
    "        title = ''\n",
    "        title_element = soup.title\n",
    "    \n",
    "        # no title found\n",
    "        if title_element is None or len(title_element) == 0:\n",
    "            print(\"Error\")\n",
    "            return title\n",
    "    \n",
    "        # title elem found\n",
    "        title_text = title_element.text\n",
    "        used_delimeter = False\n",
    "    \n",
    "    #     title from h1\n",
    "            # - extract the longest text from all h1 elements\n",
    "        # - too short texts (fewer than 2 words) are discarded\n",
    "        # - clean double spaces\n",
    "    #     h1_element = soup.find_all('h1')[0]\n",
    "    #     title_text_h1 = h1_element.text\n",
    "        title_text_h1=''\n",
    "        title_element_h1_list = soup.find_all('h1')\n",
    "        title_text_h1_list = [tag.get_text(strip=True) for tag in title_element_h1_list]\n",
    "        if title_text_h1_list:\n",
    "            title_text_h1_list.sort(key=len, reverse=True)\n",
    "            #longest title\n",
    "            title_text_h1 = title_text_h1_list[0]\n",
    "            # clean double spaces\n",
    "            title_text_h1 = ' '.join([x for x in title_text_h1.split() if x])\n",
    "        #title from meta tag(not user-visible)\n",
    "        meta_tag_content = soup.find({'meta': {'property': 'og:title'}})\n",
    "        if not meta_tag_content:\n",
    "            meta_tag_content = soup.find({'meta': {'name': 'og:title'}})\n",
    "        title_text_meta = meta_tag_content.get('content', '')  # Empty string if no meta tag found\n",
    "        # Further filtering of unwanted characters\n",
    "        # Alphanumeric characters, punctuation and alphanumeric\n",
    "        filter_regex = re.compile(r'[^a-zA-Z0-9\\ ]')\n",
    "        filter_title_text = filter_regex.sub('', title_text).lower()\n",
    "        filter_title_text_h1 = filter_regex.sub('', title_text_h1).lower()\n",
    "        filter_title_text_meta = filter_regex.sub('', title_text_meta).lower()\n",
    "        \n",
    "        # Case1: If both matches don't do anything\n",
    "        if title_text_h1 == title_text:\n",
    "            used_delimeter = True\n",
    "        # Case2: h1 and meta tag matches(either of h1 or meta)\n",
    "        elif filter_title_text_h1 and filter_title_text_h1 == filter_title_text_meta:\n",
    "            title_text = title_text_h1\n",
    "            used_delimeter = True\n",
    "        # Case3: If both h1 and meta are a substring of title_text(use h1)\n",
    "        elif filter_title_text_h1 and filter_title_text_h1 in filter_title_text and filter_title_text_meta in filter_title_text  and len(title_text_h1) > len(title_text_meta):\n",
    "            title_text = title_text_h1\n",
    "            used_delimeter = True\n",
    "        # Case4: If title_text startswith meta text(replace with meta)\n",
    "        elif filter_title_text_meta and filter_title_text_meta != filter_title_text and filter_title_text.startswith(filter_title_text_meta):\n",
    "            title_text = title_text_meta\n",
    "            used_delimeter = True\n",
    "        \n",
    "        # If none of the above condition is matched, means a delimiter must be present between them\n",
    "        # Now individually parts separated by delimiter has to be checked and now we check with h1 tag only(no meta tag)-Observation based\n",
    "        if not used_delimeter and '|' in title_text:\n",
    "            title_text = self.split_title(title_text, '|', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with -\n",
    "        if not used_delimeter and '-' in title_text:\n",
    "            title_text = self.split_title(title_text, '-', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with _\n",
    "        if not used_delimeter and '_' in title_text:\n",
    "            title_text = self.split_title(title_text, '_', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with /\n",
    "        if not used_delimeter and '/' in title_text:\n",
    "            title_text = self.split_title(title_text, '/', title_text_h1)\n",
    "            used_delimeter = True\n",
    "    \n",
    "        # self.split title with »\n",
    "        if not used_delimeter and ' » ' in title_text:\n",
    "            title_text = self.split_title(title_text, ' » ', title_text_h1)\n",
    "            used_delimeter = True\n",
    "        return title_text\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDt5gevywMRA",
    "outputId": "2ee5c5f8-a1e9-42ce-dcba-260327dbf757"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'cnalifestyle.channelnewsasia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors  are\n",
      "Jiamun Koh\n",
      "22 Jan 2024 10:01AM(Updated: 22 Jan 2024 03:53PM)\n",
      "date of publication: 2024-01-22 10:01:00\n",
      "title  is/are\n",
      "Actress Nurul Aini responds to viral video of husband allegedly at hotel with another actress\n",
      "\n",
      "\n",
      "\n",
      "Best Node: ['dialog-off-canvas-main-canvas'], Gravity Score: 7789.21875\n",
      "Edition MenuEdition:Go to CNACNA LuxuryHamburger MenuCloseMain navigationEntertainmentEntertainmentEntertainmentZoe Tay celebrates her 56th birthday with residents from Lee Ah Mooi Old Age HomeEntertainmentAdditional tickets for Bruno Mars, Taylor Swift concerts in Singapore to go on sale on Jan 25EntertainmentActress Nurul Aini responds to viral video of husband checking into hotel with another actressEntertainmentYES 933 DJ Hazelle Teo spent S$40,000 to produce her Chinese New Year songCelebrityCulture & TrendsMusicTelevision & MoviesWomenWomenWomenHow Malay bridal entrepreneur Yumi Ayummi became a hit among millennials and Gen Z newlyweds-to-beWomenMeet the 39-year-old professional dancer who never let Down syndrome stop herWomenThe Singaporean artist who once turned an HDB staircase gold continues to highlight social issues through her artWomenDancing with Down syndrome: ‘I just want to continue dancing for a long time’Women's HealthWomen's LifeWomen's StyleWomen's VoicesWellnessWellnessWellnessBottled water is full of plastic particles – can they harm your health?WellnessThe 7 keys to longevity: Evidence-backed tips on how to add more good years to your lifeWellnessWork Well: How to keep calm and make a good impression during a work presentationWellnessCan dieting actually lead to long-term weight loss?FitnessHealthWellbeingLivingLivingPeopleThe architect and designer exploring Korean craft and heritage through different mediums, including skincareObsessions13 furniture designs from Hermes, Louis Vuitton and more that add a touch of luxury to your homeBuyers' Guide9 essential kitchen appliances and tools to help with Chinese New Year cookingLivingSamsung packs newest Galaxy S24 smartphones with AI functions to beat AppleHomesCareerMoneyTechStyle & BeautyStyle & BeautyStyle & BeautyShe’s collected over 500 pieces of vintage clothes, including cheongsams: ‘Fashion is cyclical, your taste is not’ObsessionsItalian luxury brand Tod’s drops first-ever collaboration with LamborghiniStyle & BeautyCollectors Club: A Singaporean’s love for cheongsam and vintage clothesPeopleThe architect and designer exploring Korean craft and heritage through different mediums, including skincareBeautyFashionAccessoriesMenDiningDiningDiningOur picks of where to get top-quality sushi for less in Singapore with these restaurants’ lunch offersDiningThese NUS undergrads’ new mee hoon kueh stall at Bishan bus interchange is attracting queuesExperiencesDelicious ways to usher in the Chinese New YearDiningPopular Meng Meng Roasted Duck from JB opens full-service restaurant in Singapore with zi char dishesFood & DrinksRestaurantsChefsTravelTravelExperiencesPhuket by design: 6 luxury resorts with striking architecture and interiorsExperiencesAt this Bordeaux wine estate, the hogs eat Michelin-starred scraps and ducks are friends, not foodAsiaSakura forecast for 2024: Here's when you can catch cherry blossoms in JapanExperiences‘Villeggiatura’ and ‘dry-tripping’: Here are 12 trends shaping how we’ll travel in 2024Places & AttractionsActivitiesWeekend EscapesChinese New YearFollow UsFacebookTwitterYoutubeInstagramSign InAccountSearchEntertainmentActress Nurul Aini responds to viral video of husband allegedly at hotel with another actressBookmarkBookmarkShareWhatsAppTelegramFacebookTwitterEmailLinkedInAdvertisementEntertainmentActress Nurul Aini responds to viral video of husband allegedly at hotel with another actressNurul Aini's property agent husband Sofian Roslan, who was allegedly seen with Suria actress Fatin Amira, has since admitted to the scandal on his Instagram, claiming that he wishes to spend the rest of his life \"making it up\" to his wife and their family.Nurul Aini (right) responds to the viral video of her husband Sofian Roslan (left) checking into a hotel with an actress. (Photos: Instagram/aanurul, TikTok/mzkmzk54)New: You can now listen to articles.Sorry, the audio is unavailable right now.Please try again later.This audio is AI-generated.Jiamun KohJiamun Koh22 Jan 2024 10:01AM(Updated: 22 Jan 2024 03:53PM)BookmarkBookmarkShareWhatsAppTelegramFacebookTwitterEmailLinkedInSaturday morning (Jan 20), a video of Singaporean actress Nurul Aini's husband allegedly cheating on her was shared online.The caption of the video read: “Nurul Aini husband Sofian Roslan checks in hotel with Fatin Amira”The video, which started with the words “Is Nurul Aini’s husband having an affair?”, showed a man said to be Nurul's property agent husband Sofian Roslan and a woman, who is allegedly Suria actress Fatin Amira, coming out of the same Mercedes-Benz separately.A man said to be Nurul's husband and a woman, who is allegedly actress Fatin Amira, seen getting out of the same car. (Photo: TikTok/mzkmzk54)The car was parked in what is assumed to be an underground carpark.The pair were then shown entering the Capri By Fraser China Square hotel lift lobby moments apart from one another.The pair are seen entering the Capri By Fraser China Square hotel lift lobby. (Photo: TikTok/mzkmzk54)The video ended with the man and woman returning to their parked car separately.After both board the vehicle, they were seen kissing each other on the lips.The woman is seen leaning in for a kiss after they both are back in the car. (Photo: TikTok/mzkmzk54)A few hours after the video was shared online, Nurul, 41, issued a statement on her socials.\"This is an incredibly difficult time for my family and me. We kindly request understanding and privacy as we navigate through these challenges. Your support & prayers are very much appreciated. Thank you,\" said the actress.8days.sg reached out to Nurul, who declined to comment on the scandal.View this post on InstagramA post shared by Nurul Aini (@aanurul)Sofian, on the other hand, took to his Instagram to admit to his infidelity.Starting his apology with the quote \"Now I can't go on without you, and I'm not that strong without you\", Sofian went on to write: \"My dearest beautiful wife Nurul Aini, 24 years together. 16 years of marriage. 3 beautiful children. Allah blessed me with so much and you always reminded me to cherish what we have.\"\"But I faltered and I lost my way. I can’t even imagine the amount of pain I’ve caused you and our family,\" he said regretfully.Nurul and Sofian have a 12-year-old son and two daughters, who are 11 and two.He continued: \"I am sorry B. You mean everything to me. Everything. And if you’ll let me, I want to spend the rest of my life making it up to you and our children. I love all of you so much.\"Sofian also apologised to his family, friends, clients and followers for his \"shortcomings\" and asked for them to give him and Nurul space to \"get through this difficult time to heal properly\".However, the post has been deleted by Sofian one day after it was shared on his Instagram (Jan 21).Meanwhile, Fatin, who's allegedly also married and a property agent, has not responded to the scandal.She has, however, set her Instagram to private ever since the video went viral.This story was originallypublishedin 8Days.For more 8Days stories, visithttps://www.8days.sg/Source: 8 Days/ktRelated TopicscelebrityAdvertisementRECOMMENDEDContent is loading...AdvertisementExpand to read the full storyRecent SearchesTrending TopicscelebrityhomesfashionHealthWomen's Voicesfood & drinkPlaces & AttractionsCopyright© Mediacorp 2024. Mediacorp Pte Ltd. All rights reserved.About Mediacorp|Terms & Conditions|Privacy PolicyFollow UsFacebookTwitterYoutubeInstagramThis browser is no longer supportedWe know it's a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be.To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.Upgraded but still having issues?Contact us\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "url=\"https://cnalifestyle.channelnewsasia.com/entertainment/nurul-aini-sofian-roslan-fatin-amira-suria-actress-scandal-383591#mdcrecs_s\"\n",
    "proxies = {\n",
    "\"http\": \"http://scraperapi:8355bf750256f87924cb321115d06996@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url,headers=headers,proxies=proxies,verify=False)\n",
    "API_KEY = \"8355bf750256f87924cb321115d06996\"\n",
    "params = {'api_key': API_KEY, 'url': url}\n",
    "response = requests.get('http://api.scraperapi.com/', params=urlencode(params))\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# print(soup)\n",
    "# Create an instance of CustomExtractor\n",
    "extractor = CustomExtractor()\n",
    "\n",
    "\n",
    "authors_list=extractor.get_authors(soup);\n",
    "print(\"authors  are\")\n",
    "\n",
    "if authors_list:\n",
    "    \n",
    "    for author in authors_list:\n",
    "        print(author)\n",
    "       \n",
    "else:\n",
    "    print(\"No authors found.\")\n",
    "date=extractor.publishing_date(url,soup)\n",
    "\n",
    "print(\"date of publication:\",date)\n",
    "\n",
    "title_list=extractor.get_title(soup);\n",
    "print(\"title  is/are\")\n",
    "\n",
    "print(extractor.get_title(soup))\n",
    "\n",
    "\n",
    "# Calculate best node based on custom gravity scores\n",
    "best_node = extractor.calculate_best_node(soup)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "# Now, you can access the best node and its gravity score\n",
    "if best_node:\n",
    "    print(f\"Best Node: {best_node['class']}, Gravity Score: {best_node.get('score', 0)}\")\n",
    "    print(best_node.get_text(strip=True))\n",
    "else:\n",
    "    print(\"No best node found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLHXth5VCnoBeFhz4tcqmD",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
